# Python `@dataclass` as Configuration

Python’s `@dataclass` decorator is a powerful tool for creating structured and type-safe configuration classes. By defining configurations as `dataclass` objects, you can easily manage and validate the parameters for your scripts or machine learning jobs.

In this tutorial, you’ll learn:

- How to define and use a configuration class

- Adding default values and type hints

- Validating configurations with `@dataclass`

- Using nested configurations for complex setups

---

## 1. Defining and Using a Configuration Class

To start, let’s define a simple configuration class for a PyTorch model training function using `@dataclass`:

```python
from dataclasses import dataclass
import torch

@dataclass
class Config:
    seed: int = 42
    epochs: int = 10
    batch_size: int = 32
    learning_rate: float = 0.001
    weight_decay: float = 1e-5
    model_path: str = "./checkpoints/best.pt"
    data_dir: str = "./datasets"
    device: str = "cuda"
    early_stopping: bool = False
    patience: int = 3
```

This configuration class defines all the parameters required for training. Default values are provided, but you can override them when initializing the class.

---

## 2. Using the Configuration in Training

Here’s an example of how to use the `Config` class in a PyTorch training script:

```python
def load_model(model_name: str):
    # Load the model
    pass

def train_model(model, dataloaders, optimizer, lr_scheduler, config: Config):
    model = model.to(config.device)
    for epoch in range(config.epochs):
        # Training logic
        pass

def main():
    # Initialize configuration
    config = Config(batch_size=64, learning_rate=0.01, early_stopping=True, patience=5)

    dataloaders = {}
    model = load_model("resnet50")
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 5], gamma=0.1)

    # Train the model
    best_model, val_acc = train_model(model, dataloaders, optimizer, lr_scheduler, config)

if __name__ == "__main__":
    main()
```

---

## 3. Benefits of `@dataclass`

### 3.1 Readability

The @dataclass provides a clear, structured representation of configuration parameters.

```python
@dataclass
class Config:
    seed: int
    epochs: int
    ...
```

### 3.2 Default Values

Default values can be set directly when defining fields.

```python
learning_rate: float = 0.001
```

### 3.3 Type Safety

Type hints ensure parameters are used correctly, reducing bugs and improving editor support.

```python
device: str  # Accepts "cpu" or "cuda"
```

### 3.4 Validation

Validation logic can be added using the __post_init__ method.

```python
@dataclass
class Config:
    batch_size: int
    epochs: int
    learning_rate: float

    def __post_init__(self):
        if self.batch_size <= 0:
            raise ValueError("Batch size must be positive.")
        if self.epochs <= 0:
            raise ValueError("Epochs must be positive.")
        if not (0 < self.learning_rate <= 1):
            raise ValueError("Learning rate must be in the range (0, 1].")
```

## 4. Advanced: Nested Configurations

For more complex setups, you can nest `dataclass` objects.

```python
@dataclass
class OptimizerConfig:
    learning_rate: float = 0.001
    weight_decay: float = 1e-5

@dataclass
class Config:
    seed: int = 42
    epochs: int = 10
    optimizer: OptimizerConfig = OptimizerConfig()

```

Usage:

```python
config = Config(optimizer=OptimizerConfig(learning_rate=0.01, weight_decay=0.01))
```

---

## 5. Summary

In this tutorial, you learned how to use Python’s @dataclass decorator to define and manage configurations for your PyTorch training scripts. This approach enhances readability, type safety, and reusability.

Next, explore how to use Weights & Biases to track and visualize your training runs. See [WandB](./5.WandB.md).
